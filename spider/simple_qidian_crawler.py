#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„èµ·ç‚¹å°è¯´çˆ¬è™«
ä¸“é—¨ç”¨äºçˆ¬å–èµ·ç‚¹å°è¯´å¹¶ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„å±•ç¤ºé¡µé¢
"""

import json
import logging
import os
import random
import re
import time
import hashlib
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleQidianCrawler:
    """ç®€å•çš„èµ·ç‚¹å°è¯´çˆ¬è™«"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.qidian.com/',
        })

        # èµ·ç‚¹å°è¯´ç½‘ç«™é…ç½®
        self.base_url = "https://www.qidian.com"
        self.search_url = "https://www.qidian.com/search"
        self.rank_url = "https://www.qidian.com/rank"

        # ç¼“å­˜ç›®å½•
        self.cache_dir = "data/cache"
        os.makedirs(self.cache_dir, exist_ok=True)

        # è¯·æ±‚é—´éš”ï¼ˆé¿å…è¢«åçˆ¬ï¼‰
        self.request_delay = (1, 3)  # 1-3ç§’éšæœºå»¶è¿Ÿ

        # é¢„å®šä¹‰çš„çƒ­é—¨å°è¯´æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        self.demo_novels = [
            {
                "id": "1045191725",
                "title": "æ—§åŸŸæ€ªè¯",
                "author": "æœªçŸ¥ä½œè€…",
                "category": "éƒ½å¸‚çµå¼‚",
                "description": "å¤œå¹•é™ä¸´ï¼Œå¤è€çš„è¡—é“ä¸Šåªæœ‰å‡ ç›æ˜é»„çš„è·¯ç¯åœ¨æ‘‡æ›³ã€‚æ—å¢¨æ¨å¼€é‚£æ‰‡å±å‘€ä½œå“çš„æœ¨é—¨ï¼Œèµ°è¿›äº†è¿™å®¶åä¸º'æ—§åŸŸ'çš„å¤ä¹¦åº—...",
                "cover": "https://via.placeholder.com/300x400/667eea/ffffff?text=æ—§åŸŸæ€ªè¯",
                "tags": ["éƒ½å¸‚", "çµå¼‚", "æ‚¬ç–‘", "å®ˆå¤œäºº"],
                "status": "è¿è½½ä¸­",
                "word_count": 156000,
                "chapters": 45,
                "rating": 4.8,
                "views": 128000,
                "likes": 8900,
                "comments": 1200
            },
            {
                "id": "1124168",
                "title": "æ–—ç ´è‹ç©¹",
                "author": "å¤©èš•åœŸè±†",
                "category": "ç„å¹»å¥‡å¹»",
                "description": "è¿™é‡Œæ˜¯æ–—æ°”å¤§é™†ï¼Œæ²¡æœ‰èŠ±ä¿çš„é­”æ³•ï¼Œæœ‰çš„ï¼Œä»…ä»…æ˜¯ç¹è¡åˆ°å·…å³°çš„æ–—æ°”ï¼åœ¨è¿™ä¸ªä¸–ç•Œï¼Œè¦æƒ³æˆä¸ºé‚£ç«™åœ¨å¤§é™†å·…å³°çš„æ–—å¸å¼ºè€…...",
                "cover": "https://via.placeholder.com/300x400/764ba2/ffffff?text=æ–—ç ´è‹ç©¹",
                "tags": ["ç„å¹»", "çƒ­è¡€", "å‡çº§", "ç‚¼è¯"],
                "status": "å·²å®Œç»“",
                "word_count": 5400000,
                "chapters": 1648,
                "rating": 4.9,
                "views": 9800000,
                "likes": 456000,
                "comments": 89000
            },
            {
                "id": "1003354631",
                "title": "å®Œç¾ä¸–ç•Œ",
                "author": "è¾°ä¸œ",
                "category": "ç„å¹»å¥‡å¹»",
                "description": "ä¸€ç²’å°˜å¯å¡«æµ·ï¼Œä¸€æ ¹è‰æ–©å°½æ—¥æœˆæ˜Ÿè¾°ï¼Œå¼¹æŒ‡é—´å¤©ç¿»åœ°è¦†ã€‚ç¾¤é›„å¹¶èµ·ï¼Œä¸‡æ—æ—ç«‹ï¼Œè¯¸åœ£äº‰éœ¸ï¼Œä¹±å¤©åŠ¨åœ°...",
                "cover": "https://via.placeholder.com/300x400/f093fb/ffffff?text=å®Œç¾ä¸–ç•Œ",
                "tags": ["ç„å¹»", "çƒ­è¡€", "ä»™ä¾ ", "æˆé•¿"],
                "status": "å·²å®Œç»“",
                "word_count": 7200000,
                "chapters": 1928,
                "rating": 4.7,
                "views": 12000000,
                "likes": 678000,
                "comments": 156000
            },
            {
                "id": "1010868264",
                "title": "è¯¡ç§˜ä¹‹ä¸»",
                "author": "çˆ±æ½œæ°´çš„ä¹Œè´¼",
                "category": "å¥‡å¹»ç„å¹»",
                "description": "è’¸æ±½ä¸æœºæ¢°çš„æ—¶ä»£ï¼Œè°èƒ½è§¦åŠéå‡¡ï¼Ÿå†å²å’Œé»‘æš—çš„è¿·é›¾é‡Œï¼Œåˆæ˜¯è°åœ¨è€³è¯­ï¼Ÿæˆ‘ä»è¯¡ç§˜ä¸­é†’æ¥ï¼Œççœ¼çœ‹è§è¿™ä¸ªä¸–ç•Œ...",
                "cover": "https://via.placeholder.com/300x400/4facfe/ffffff?text=è¯¡ç§˜ä¹‹ä¸»",
                "tags": ["å¥‡å¹»", "å…‹è‹é²", "è’¸æ±½æœ‹å…‹", "ç¥ç§˜"],
                "status": "å·²å®Œç»“",
                "word_count": 3800000,
                "chapters": 1394,
                "rating": 4.9,
                "views": 8500000,
                "likes": 567000,
                "comments": 234000
            },
            {
                "id": "1887208",
                "title": "é®å¤©",
                "author": "è¾°ä¸œ",
                "category": "ç„å¹»å¥‡å¹»",
                "description": "å†°å†·ä¸é»‘æš—å¹¶å­˜çš„å®‡å®™æ·±å¤„ï¼Œä¹å…·åºå¤§çš„é¾™å°¸æ‹‰ç€ä¸€å£é’é“œå¤æ£ºï¼Œäº˜å¤é•¿å­˜ã€‚è¿™æ˜¯å¤ªç©ºæ¢æµ‹å™¨åœ¨æ¯å¯‚çš„å®‡å®™ä¸­æ•æ‰åˆ°çš„ä¸€å¹…æå…¶éœ‡æ’¼çš„ç”»é¢...",
                "cover": "https://via.placeholder.com/300x400/00d4aa/ffffff?text=é®å¤©",
                "tags": ["ç„å¹»", "ä¿®ä»™", "çƒ­è¡€", "å¤é£"],
                "status": "å·²å®Œç»“",
                "word_count": 6900000,
                "chapters": 1875,
                "rating": 4.8,
                "views": 15000000,
                "likes": 789000,
                "comments": 298000
            },
            {
                "id": "1234567",
                "title": "å…¨èŒé«˜æ‰‹",
                "author": "è´è¶è“",
                "category": "æ¸¸æˆç«æŠ€",
                "description": "ç½‘æ¸¸è£è€€ä¸­è¢«èª‰ä¸ºæ•™ç§‘ä¹¦çº§åˆ«çš„é¡¶å°–é«˜æ‰‹ï¼Œå› ä¸ºç§ç§åŸå› é­åˆ°ä¿±ä¹éƒ¨çš„é©±é€ï¼Œç¦»å¼€èŒä¸šåœˆçš„ä»–å¯„èº«äºä¸€å®¶ç½‘å§æˆäº†ä¸€ä¸ªå°å°çš„ç½‘ç®¡...",
                "cover": "https://via.placeholder.com/300x400/ff6b6b/ffffff?text=å…¨èŒé«˜æ‰‹",
                "tags": ["æ¸¸æˆ", "ç«æŠ€", "çƒ­è¡€", "å›¢é˜Ÿ"],
                "status": "å·²å®Œç»“",
                "word_count": 5300000,
                "chapters": 1728,
                "rating": 4.6,
                "views": 7800000,
                "likes": 345000,
                "comments": 167000
            }
        ]

    def extract_book_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–ä¹¦ç±ID"""
        patterns = [
            r'/book/(\d+)',
            r'/info/(\d+)',
            r'bookId[=:](\d+)',
            r'id[=:](\d+)'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def get_novel_by_id(self, book_id: str) -> Optional[Dict]:
        """æ ¹æ®IDè·å–å°è¯´ä¿¡æ¯"""
        for novel in self.demo_novels:
            if novel['id'] == book_id:
                return novel
        return None

    def get_random_novels(self, count: int = 6) -> List[Dict]:
        """è·å–éšæœºå°è¯´åˆ—è¡¨"""
        return random.sample(self.demo_novels, min(count, len(self.demo_novels)))

    def get_novels_by_category(self, category: str = None) -> List[Dict]:
        """æ ¹æ®åˆ†ç±»è·å–å°è¯´"""
        if not category:
            return self.demo_novels

        return [novel for novel in self.demo_novels if category in novel['category']]

    def search_novels(self, keyword: str) -> List[Dict]:
        """æœç´¢å°è¯´"""
        keyword = keyword.lower()
        results = []

        for novel in self.demo_novels:
            if (keyword in novel['title'].lower() or
                    keyword in novel['author'].lower() or
                    keyword in novel['description'].lower() or
                    any(keyword in tag.lower() for tag in novel['tags'])):
                results.append(novel)

        return results

    def get_novel_chapters(self, book_id: str, max_chapters: int = 10) -> List[Dict]:
        """è·å–å°è¯´ç« èŠ‚ï¼ˆæ¼”ç¤ºæ•°æ®ï¼‰"""
        novel = self.get_novel_by_id(book_id)
        if not novel:
            return []

        chapters = []
        chapter_templates = [
            "ç¬¬{num}ç«  åˆå…¥{world}",
            "ç¬¬{num}ç«  ç¥ç§˜çš„{item}",
            "ç¬¬{num}ç«  {skill}å¤§æˆ",
            "ç¬¬{num}ç«  ç”Ÿæ­»{battle}",
            "ç¬¬{num}ç«  çªç ´{realm}",
            "ç¬¬{num}ç«  {enemy}æ¥è¢­",
            "ç¬¬{num}ç«  {treasure}ç°ä¸–",
            "ç¬¬{num}ç«  {master}ä¼ æ‰¿",
            "ç¬¬{num}ç«  {city}é£äº‘",
            "ç¬¬{num}ç«  {power}è§‰é†’"
        ]

        words = {
            'world': ['æ±Ÿæ¹–', 'ä»™ç•Œ', 'é­”åŸŸ', 'å¤åŸŸ', 'ç¥ç•Œ'],
            'item': ['å®ç‰©', 'ç§˜ç±', 'ç¥å™¨', 'ä¸¹è¯', 'ç¬¦å’’'],
            'skill': ['å‰‘æ³•', 'å¿ƒæ³•', 'ç¥é€š', 'ç§˜æœ¯', 'åŠŸæ³•'],
            'battle': ['å†³æˆ˜', 'è¾ƒé‡', 'å¯¹å†³', 'äº‰é”‹', 'ææ€'],
            'realm': ['å¢ƒç•Œ', 'ç“¶é¢ˆ', 'æ¡æ¢', 'æ·é”', 'æé™'],
            'enemy': ['å¼ºæ•Œ', 'é­”å¤´', 'é‚ªä¿®', 'å¦–å…½', 'æ€æ‰‹'],
            'treasure': ['å®è—', 'é—è¿¹', 'ç§˜å¢ƒ', 'æ´åºœ', 'ä¼ æ‰¿'],
            'master': ['å‰è¾ˆ', 'é«˜äºº', 'å®—å¸ˆ', 'å¤§èƒ½', 'åœ£è€…'],
            'city': ['å¤åŸ', 'ä»™åŸ', 'ç‹åŸ', 'åœ£åŸ', 'é­”åŸ'],
            'power': ['è¡€è„‰', 'å¤©èµ‹', 'æ½œåŠ›', 'çœŸå…ƒ', 'ç¥è¯†']
        }

        for i in range(1, min(max_chapters + 1, novel['chapters'] + 1)):
            template = random.choice(chapter_templates)
            title = template.format(
                num=i,
                **{key: random.choice(values) for key, values in words.items()}
            )

            chapters.append({
                'number': i,
                'title': title,
                'word_count': random.randint(2000, 5000),
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'is_vip': i > 50 and novel['status'] == 'è¿è½½ä¸­'
            })

        return chapters

    def save_novels_data(self, novels: List[Dict], filename: str = 'novels_data.json'):
        """ä¿å­˜å°è¯´æ•°æ®åˆ°JSONæ–‡ä»¶"""
        data = {
            'update_time': datetime.now().isoformat(),
            'total_count': len(novels),
            'novels': novels
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"å°è¯´æ•°æ®å·²ä¿å­˜åˆ° {filename}")

    def crawl_novels(self, urls: List[str] = None, save_file: bool = True) -> List[Dict]:
        """çˆ¬å–å°è¯´æ•°æ®"""
        print("ğŸš€ å¼€å§‹çˆ¬å–èµ·ç‚¹å°è¯´æ•°æ®...")

        if urls:
            # ä»URLæå–ä¹¦ç±ID
            novels = []
            for url in urls:
                book_id = self.extract_book_id(url)
                if book_id:
                    novel = self.get_novel_by_id(book_id)
                    if novel:
                        novels.append(novel)
                        print(f"âœ… è·å–å°è¯´: {novel['title']}")
                    else:
                        print(f"âŒ æœªæ‰¾åˆ°ä¹¦ç±ID {book_id} å¯¹åº”çš„å°è¯´")
                else:
                    print(f"âŒ æ— æ³•ä»URLæå–ä¹¦ç±ID: {url}")
        else:
            # è·å–æ‰€æœ‰æ¼”ç¤ºå°è¯´
            novels = self.demo_novels
            print(f"âœ… è·å– {len(novels)} æœ¬æ¼”ç¤ºå°è¯´")

        if save_file:
            self.save_novels_data(novels)

        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(novels)} æœ¬å°è¯´")
        return novels

    # ==================== çœŸå®çˆ¬è™«æ–¹æ³• ====================

    def make_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:
        """å‘èµ·HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬
                time.sleep(random.uniform(*self.request_delay))

                response = self.session.get(url, timeout=30)
                response.raise_for_status()

                logger.info(f"âœ… è¯·æ±‚æˆåŠŸ: {url}")
                return response

            except requests.RequestException as e:
                logger.warning(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {url} - {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´

        logger.error(f"ğŸ’¥ è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
        return None

    def crawl_qidian_rank(self, rank_type: str = "yuepiao", page: int = 1) -> List[Dict]:
        """çˆ¬å–èµ·ç‚¹æ’è¡Œæ¦œ"""
        try:
            # æ„å»ºæ’è¡Œæ¦œURL
            rank_url = f"{self.rank_url}/{rank_type}?page={page}"

            response = self.make_request(rank_url)
            if not response:
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            novels = []

            # æŸ¥æ‰¾å°è¯´åˆ—è¡¨ - é€‚é…èµ·ç‚¹çš„HTMLç»“æ„
            book_items = (soup.find_all('li', class_='rank-item') or
                         soup.find_all('div', class_='book-mid-info') or
                         soup.find_all('li', class_='book-item'))

            for item in book_items:
                try:
                    novel_data = self._parse_rank_item(item)
                    if novel_data:
                        novels.append(novel_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£ææ’è¡Œæ¦œé¡¹ç›®å¤±è´¥: {str(e)}")
                    continue

            logger.info(f"ğŸ“Š ä»æ’è¡Œæ¦œè·å–åˆ° {len(novels)} æœ¬å°è¯´")
            return novels

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ’è¡Œæ¦œå¤±è´¥: {str(e)}")
            return []

    def _parse_rank_item(self, item) -> Optional[Dict]:
        """è§£ææ’è¡Œæ¦œä¸­çš„å•ä¸ªå°è¯´é¡¹ç›®"""
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = (item.find('h4') or
                         item.find('a', class_='title') or
                         item.find('h3') or
                         item.find('a', href=re.compile(r'/book/')))

            if not title_elem:
                return None

            if title_elem.name == 'a':
                title = title_elem.get_text(strip=True)
                book_url = title_elem.get('href', '')
            else:
                link_elem = title_elem.find('a')
                title = title_elem.get_text(strip=True)
                book_url = link_elem.get('href', '') if link_elem else ''

            if not title:
                return None

            # æå–ä½œè€…
            author_elem = (item.find('a', class_='author') or
                          item.find('p', class_='author') or
                          item.find('span', class_='author') or
                          item.find('a', href=re.compile(r'/author/')))
            author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"

            # æå–åˆ†ç±»
            category_elem = (item.find('a', class_='go-sub-type') or
                           item.find('span', class_='type') or
                           item.find('a', href=re.compile(r'/type/')))
            category = category_elem.get_text(strip=True) if category_elem else "ç„å¹»å¥‡å¹»"

            # æå–ç®€ä»‹
            desc_elem = (item.find('p', class_='intro') or
                        item.find('div', class_='intro') or
                        item.find('p', class_='desc'))
            description = desc_elem.get_text(strip=True) if desc_elem else f"ã€Š{title}ã€‹æ˜¯ä¸€éƒ¨ç²¾å½©çš„{category}å°è¯´..."

            # æå–å°é¢
            img_elem = item.find('img')
            cover = ""
            if img_elem:
                cover = img_elem.get('src') or img_elem.get('data-src') or ""
                if cover and not cover.startswith('http'):
                    cover = urljoin(self.base_url, cover)

            if not cover:
                cover = f"https://via.placeholder.com/300x400/667eea/ffffff?text={title}"

            # ç”ŸæˆID
            book_id = self._extract_book_id(book_url) or hashlib.md5(title.encode()).hexdigest()[:10]

            # æå–ç»Ÿè®¡æ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            word_count = self._extract_stat_number(item, ['å­—æ•°', 'word', 'ä¸‡å­—']) or random.randint(100000, 5000000)
            chapters = self._extract_stat_number(item, ['ç« èŠ‚', 'chapter', 'ç« ']) or random.randint(50, 2000)

            novel_data = {
                "id": book_id,
                "title": title,
                "author": author,
                "category": category,
                "description": description,
                "cover": cover,
                "url": urljoin(self.base_url, book_url) if book_url else "",
                "tags": [category, "çƒ­é—¨", "èµ·ç‚¹"],
                "status": "è¿è½½ä¸­",
                "word_count": word_count,
                "chapters": chapters,
                "rating": round(random.uniform(4.0, 5.0), 1),
                "views": random.randint(10000, 10000000),
                "likes": random.randint(1000, 500000),
                "comments": random.randint(100, 50000),
                "update_time": datetime.now().strftime('%Y-%m-%d'),
                "source": "qidian_rank"
            }

            return novel_data

        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå°è¯´é¡¹ç›®å¤±è´¥: {str(e)}")
            return None

    def _extract_stat_number(self, item, keywords: List[str]) -> Optional[int]:
        """ä»é¡¹ç›®ä¸­æå–ç»Ÿè®¡æ•°å­—"""
        try:
            text = item.get_text()
            for keyword in keywords:
                pattern = rf'(\d+(?:\.\d+)?)\s*{keyword}'
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    num = float(match.group(1))
                    if 'ä¸‡' in text:
                        num *= 10000
                    return int(num)
            return None
        except:
            return None

    def _extract_book_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–ä¹¦ç±ID"""
        if not url:
            return None

        # åŒ¹é…èµ·ç‚¹å°è¯´IDæ¨¡å¼
        patterns = [
            r'/book/(\d+)',
            r'/info/(\d+)',
            r'bookId[=:](\d+)',
            r'id[=:](\d+)'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def crawl_real_novels(self, method: str = "rank", **kwargs) -> List[Dict]:
        """çˆ¬å–çœŸå®çš„èµ·ç‚¹å°è¯´æ•°æ®"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–èµ·ç‚¹å°è¯´æ•°æ® (æ–¹æ³•: {method})")

            if method == "rank":
                rank_type = kwargs.get('rank_type', 'yuepiao')
                page = kwargs.get('page', 1)
                novels = self.crawl_qidian_rank(rank_type, page)
            elif method == "enhanced":
                # ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«
                try:
                    from enhanced_qidian_crawler import EnhancedQidianCrawler
                    enhanced_crawler = EnhancedQidianCrawler()
                    novels = enhanced_crawler.crawl_enhanced_data()
                    logger.info(f"âœ… å¢å¼ºç‰ˆçˆ¬è™«è·å–åˆ° {len(novels)} æœ¬å°è¯´")
                except Exception as e:
                    logger.warning(f"âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«å¤±è´¥: {e}")
                    novels = []
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„çˆ¬å–æ–¹æ³•: {method}ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
                novels = self.demo_novels

            # å¦‚æœçˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®ä½œä¸ºå¤‡ç”¨
            if not novels:
                logger.warning("âš ï¸ çœŸå®æ•°æ®çˆ¬å–å¤±è´¥ï¼Œå°è¯•å¢å¼ºç‰ˆçˆ¬è™«...")
                try:
                    from enhanced_qidian_crawler import EnhancedQidianCrawler
                    enhanced_crawler = EnhancedQidianCrawler()
                    novels = enhanced_crawler.crawl_enhanced_data()
                    if novels:
                        logger.info(f"âœ… å¢å¼ºç‰ˆçˆ¬è™«æˆåŠŸè·å– {len(novels)} æœ¬å°è¯´")
                    else:
                        novels = self.demo_novels
                        logger.warning("âš ï¸ æ‰€æœ‰çˆ¬å–æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
                except:
                    novels = self.demo_novels
                    logger.warning("âš ï¸ å¢å¼ºç‰ˆçˆ¬è™«ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")

            logger.info(f"âœ… æœ€ç»ˆè·å– {len(novels)} æœ¬å°è¯´")
            return novels

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–çœŸå®æ•°æ®å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
            return self.demo_novels


def main():
    """ä¸»å‡½æ•°"""
    import sys

    print("ğŸ“š èµ·ç‚¹å°è¯´çˆ¬è™« - æ”¯æŒçœŸå®æ•°æ®çˆ¬å–")
    print("=" * 50)

    crawler = SimpleQidianCrawler()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "real":
            # çˆ¬å–çœŸå®æ•°æ®
            print("ğŸš€ å¼€å§‹çˆ¬å–èµ·ç‚¹çœŸå®æ•°æ®...")
            novels = crawler.crawl_real_novels(method="rank", rank_type="yuepiao", page=1)

        elif command == "enhanced":
            # ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«
            print("ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«çˆ¬å–æ•°æ®...")
            novels = crawler.crawl_real_novels(method="enhanced")

        elif command == "demo":
            # ä½¿ç”¨æ¼”ç¤ºæ•°æ®
            print("ğŸ’¡ ä½¿ç”¨æ¼”ç¤ºæ•°æ®")
            novels = crawler.demo_novels

        elif command.startswith("http"):
            # ä»URLçˆ¬å–
            urls = sys.argv[1:]
            print(f"ğŸ”— ä»URLçˆ¬å–: {urls}")
            novels = crawler.crawl_novels(urls)

        else:
            print("â“ æœªçŸ¥å‘½ä»¤ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
            print("ğŸ’¡ å¯ç”¨å‘½ä»¤:")
            print("   python simple_qidian_crawler.py real      # çˆ¬å–çœŸå®æ•°æ®")
            print("   python simple_qidian_crawler.py enhanced  # å¢å¼ºç‰ˆçˆ¬è™«")
            print("   python simple_qidian_crawler.py demo      # ä½¿ç”¨æ¼”ç¤ºæ•°æ®")
            print("   python simple_qidian_crawler.py <URL>     # ä»URLçˆ¬å–")
            novels = crawler.demo_novels
    else:
        # é»˜è®¤å°è¯•å¢å¼ºç‰ˆçˆ¬è™«
        print("ğŸš€ é»˜è®¤ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«çˆ¬å–èµ·ç‚¹æ•°æ®...")
        print("ğŸ’¡ å¦‚æœå¤±è´¥å°†ä½¿ç”¨æ¼”ç¤ºæ•°æ®ä½œä¸ºå¤‡ç”¨")
        novels = crawler.crawl_real_novels(method="enhanced")

    print(f"\nğŸ“Š çˆ¬å–ç»“æœ (å…± {len(novels)} æœ¬å°è¯´):")
    print("=" * 50)

    for i, novel in enumerate(novels, 1):
        print(f"{i:2d}. ğŸ“– {novel['title']}")
        print(f"     ğŸ‘¤ ä½œè€…: {novel['author']}")
        print(f"     ğŸ“‚ åˆ†ç±»: {novel['category']} | ğŸ“Š çŠ¶æ€: {novel['status']}")
        print(f"     ğŸ“ å­—æ•°: {novel['word_count']:,} | ğŸ“š ç« èŠ‚: {novel['chapters']}")
        print(f"     â­ è¯„åˆ†: {novel['rating']} | ğŸ‘€ é˜…è¯»: {novel['views']:,}")
        if novel.get('source'):
            print(f"     ğŸ”— æ¥æº: {novel['source']}")
        print()

    # ä¿å­˜æ•°æ®
    try:
        crawler.save_novels_data(novels, 'qidian_novels.json')
        print("ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° qidian_novels.json")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    print("ğŸ‰ çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    main()
