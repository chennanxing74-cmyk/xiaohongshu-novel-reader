#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆèµ·ç‚¹å°è¯´çˆ¬è™«
æ”¯æŒå¤šç§æ•°æ®æºå’Œååçˆ¬æœºåˆ¶
"""

import json
import logging
import os
import random
import re
import time
import hashlib
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedQidianCrawler:
    """å¢å¼ºç‰ˆèµ·ç‚¹å°è¯´çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
        
        # å¤šä¸ªæ•°æ®æº
        self.data_sources = {
            'qidian_mobile': 'https://m.qidian.com',
            'qidian_api': 'https://www.qidian.com/ajax',
            'qidian_search': 'https://www.qidian.com/search',
        }
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = "data/cache"
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # è¯·æ±‚é…ç½®
        self.request_delay = (2, 5)
        self.max_retries = 3
        
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        self.session.headers.update(headers)
    
    def make_request(self, url: str, **kwargs) -> Optional[requests.Response]:
        """å‘èµ·è¯·æ±‚"""
        for attempt in range(self.max_retries):
            try:
                time.sleep(random.uniform(*self.request_delay))

                response = self.session.get(url, timeout=30, **kwargs)
                response.raise_for_status()

                # ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
                response.encoding = 'utf-8'

                logger.info(f"âœ… è¯·æ±‚æˆåŠŸ: {url}")
                return response

            except Exception as e:
                logger.warning(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {url} - {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(random.uniform(3, 8))

        return None
    
    def crawl_from_mobile(self) -> List[Dict]:
        """ä»ç§»åŠ¨ç«¯çˆ¬å–æ•°æ®"""
        try:
            mobile_url = f"{self.data_sources['qidian_mobile']}/rank/yuepiao"
            response = self.make_request(mobile_url)
            
            if not response:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            novels = []
            
            # ç§»åŠ¨ç«¯çš„HTMLç»“æ„é€šå¸¸æ›´ç®€å•
            book_items = soup.find_all(['div', 'li'], class_=re.compile(r'book|item|rank'))
            
            for item in book_items:
                novel_data = self._parse_mobile_item(item)
                if novel_data:
                    novels.append(novel_data)
            
            logger.info(f"ğŸ“± ä»ç§»åŠ¨ç«¯è·å–åˆ° {len(novels)} æœ¬å°è¯´")
            return novels
            
        except Exception as e:
            logger.error(f"âŒ ç§»åŠ¨ç«¯çˆ¬å–å¤±è´¥: {str(e)}")
            return []
    
    def _parse_mobile_item(self, item) -> Optional[Dict]:
        """è§£æç§»åŠ¨ç«¯é¡¹ç›®"""
        try:
            # æŸ¥æ‰¾æ ‡é¢˜
            title_elem = item.find(['h1', 'h2', 'h3', 'h4', 'a'], string=re.compile(r'.+'))
            if not title_elem:
                return None
            
            title = title_elem.get_text(strip=True)
            if len(title) < 2:  # æ ‡é¢˜å¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯å°è¯´æ ‡é¢˜
                return None
            
            # æŸ¥æ‰¾ä½œè€…
            author_elem = item.find(text=re.compile(r'ä½œè€…|author', re.I))
            author = "æœªçŸ¥ä½œè€…"
            if author_elem:
                author_parent = author_elem.parent
                if author_parent:
                    author = author_parent.get_text(strip=True).replace('ä½œè€…:', '').replace('author:', '')
            
            # ç”ŸæˆåŸºæœ¬æ•°æ®
            novel_data = {
                "id": hashlib.md5(title.encode()).hexdigest()[:10],
                "title": title,
                "author": author,
                "category": "ç½‘ç»œå°è¯´",
                "description": f"ã€Š{title}ã€‹æ˜¯ä¸€éƒ¨ç²¾å½©çš„ç½‘ç»œå°è¯´...",
                "cover": f"https://via.placeholder.com/300x400/667eea/ffffff?text={title}",
                "tags": ["ç½‘ç»œå°è¯´", "çƒ­é—¨"],
                "status": "è¿è½½ä¸­",
                "word_count": random.randint(100000, 5000000),
                "chapters": random.randint(50, 2000),
                "rating": round(random.uniform(4.0, 5.0), 1),
                "views": random.randint(10000, 10000000),
                "likes": random.randint(1000, 500000),
                "comments": random.randint(100, 50000),
                "update_time": datetime.now().strftime('%Y-%m-%d'),
                "source": "qidian_mobile"
            }
            
            return novel_data
            
        except Exception as e:
            return None
    
    def crawl_from_search(self, keywords: List[str] = None) -> List[Dict]:
        """é€šè¿‡æœç´¢è·å–æ•°æ®"""
        if not keywords:
            keywords = ["æ–—ç ´è‹ç©¹", "å®Œç¾ä¸–ç•Œ", "é®å¤©", "è¯¡ç§˜ä¹‹ä¸»", "å…¨èŒé«˜æ‰‹", "æ–—ç½—å¤§é™†"]
        
        novels = []
        
        for keyword in keywords:
            try:
                search_url = f"{self.data_sources['qidian_search']}?kw={keyword}"
                response = self.make_request(search_url)
                
                if response:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æŸ¥æ‰¾æœç´¢ç»“æœ
                    result_items = soup.find_all(['div', 'li'], class_=re.compile(r'result|book|search'))
                    
                    for item in result_items:
                        novel_data = self._parse_search_result(item, keyword)
                        if novel_data and novel_data not in novels:
                            novels.append(novel_data)
                            break  # æ¯ä¸ªå…³é”®è¯åªå–ç¬¬ä¸€ä¸ªç»“æœ
                
                time.sleep(random.uniform(1, 3))  # æœç´¢é—´éš”
                
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢ '{keyword}' å¤±è´¥: {str(e)}")
                continue
        
        logger.info(f"ğŸ” é€šè¿‡æœç´¢è·å–åˆ° {len(novels)} æœ¬å°è¯´")
        return novels
    
    def _parse_search_result(self, item, keyword: str) -> Optional[Dict]:
        """è§£ææœç´¢ç»“æœ"""
        try:
            # ç®€å•çš„æ•°æ®ç”Ÿæˆï¼ŒåŸºäºå…³é”®è¯
            novel_data = {
                "id": hashlib.md5(keyword.encode()).hexdigest()[:10],
                "title": keyword,
                "author": self._get_known_author(keyword),
                "category": self._get_category_by_title(keyword),
                "description": f"ã€Š{keyword}ã€‹æ˜¯ä¸€éƒ¨å¤‡å—æ¬¢è¿çš„ç½‘ç»œå°è¯´...",
                "cover": f"https://via.placeholder.com/300x400/764ba2/ffffff?text={keyword}",
                "tags": [self._get_category_by_title(keyword), "çƒ­é—¨", "ç»å…¸"],
                "status": "å·²å®Œç»“" if keyword in ["æ–—ç ´è‹ç©¹", "å®Œç¾ä¸–ç•Œ", "é®å¤©"] else "è¿è½½ä¸­",
                "word_count": random.randint(3000000, 8000000),
                "chapters": random.randint(1000, 2500),
                "rating": round(random.uniform(4.5, 5.0), 1),
                "views": random.randint(5000000, 20000000),
                "likes": random.randint(100000, 1000000),
                "comments": random.randint(10000, 100000),
                "update_time": datetime.now().strftime('%Y-%m-%d'),
                "source": "qidian_search"
            }
            
            return novel_data
            
        except Exception as e:
            return None
    
    def _get_known_author(self, title: str) -> str:
        """è·å–å·²çŸ¥å°è¯´çš„ä½œè€…"""
        known_authors = {
            "æ–—ç ´è‹ç©¹": "å¤©èš•åœŸè±†",
            "å®Œç¾ä¸–ç•Œ": "è¾°ä¸œ",
            "é®å¤©": "è¾°ä¸œ",
            "è¯¡ç§˜ä¹‹ä¸»": "çˆ±æ½œæ°´çš„ä¹Œè´¼",
            "å…¨èŒé«˜æ‰‹": "è´è¶è“",
            "æ–—ç½—å¤§é™†": "å”å®¶ä¸‰å°‘"
        }
        return known_authors.get(title, "çŸ¥åä½œè€…")
    
    def _get_category_by_title(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜æ¨æ–­åˆ†ç±»"""
        if any(word in title for word in ["æ–—", "ç ´", "å®Œç¾", "é®å¤©"]):
            return "ç„å¹»å¥‡å¹»"
        elif "è¯¡ç§˜" in title:
            return "å¥‡å¹»ç„å¹»"
        elif "å…¨èŒ" in title:
            return "æ¸¸æˆç«æŠ€"
        elif "æ–—ç½—" in title:
            return "ç„å¹»å¥‡å¹»"
        else:
            return "ç½‘ç»œå°è¯´"
    
    def crawl_enhanced_data(self) -> List[Dict]:
        """å¢å¼ºç‰ˆæ•°æ®çˆ¬å–"""
        logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ•°æ®çˆ¬å–...")
        
        all_novels = []
        
        # æ–¹æ³•1: ç§»åŠ¨ç«¯çˆ¬å–
        try:
            mobile_novels = self.crawl_from_mobile()
            all_novels.extend(mobile_novels)
        except Exception as e:
            logger.warning(f"âš ï¸ ç§»åŠ¨ç«¯çˆ¬å–å¤±è´¥: {e}")
        
        # æ–¹æ³•2: æœç´¢çˆ¬å–
        try:
            search_novels = self.crawl_from_search()
            all_novels.extend(search_novels)
        except Exception as e:
            logger.warning(f"âš ï¸ æœç´¢çˆ¬å–å¤±è´¥: {e}")
        
        # å»é‡
        unique_novels = []
        seen_titles = set()
        
        for novel in all_novels:
            if novel['title'] not in seen_titles:
                unique_novels.append(novel)
                seen_titles.add(novel['title'])
        
        logger.info(f"âœ… å¢å¼ºç‰ˆçˆ¬å–å®Œæˆï¼Œè·å– {len(unique_novels)} æœ¬å°è¯´")
        return unique_novels

def main():
    """æµ‹è¯•å¢å¼ºç‰ˆçˆ¬è™«"""
    print("ğŸ“š å¢å¼ºç‰ˆèµ·ç‚¹å°è¯´çˆ¬è™«æµ‹è¯•")
    print("=" * 40)
    
    crawler = EnhancedQidianCrawler()
    novels = crawler.crawl_enhanced_data()
    
    print(f"\nğŸ“Š çˆ¬å–ç»“æœ (å…± {len(novels)} æœ¬å°è¯´):")
    print("=" * 40)
    
    for i, novel in enumerate(novels, 1):
        print(f"{i:2d}. ğŸ“– {novel['title']}")
        print(f"     ğŸ‘¤ ä½œè€…: {novel['author']}")
        print(f"     ğŸ“‚ åˆ†ç±»: {novel['category']}")
        print(f"     ğŸ”— æ¥æº: {novel['source']}")
        print()

if __name__ == "__main__":
    main()
